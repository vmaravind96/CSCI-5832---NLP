{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59f015d7",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ddfa87",
   "metadata": {},
   "source": [
    "## Part 1: Featurizing the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63679439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports go here\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c7f1894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the files for train data\n",
    "# Split each review into review ID and review text and store in Numpy Array\n",
    "def read_reviews_from_file(filePath):\n",
    "    \"\"\"\n",
    "    Reads the reviews from given file and performs case conversion\n",
    "    \"\"\"\n",
    "    with open(filePath) as fp:\n",
    "        reviews = fp.readlines()\n",
    "        reviews = np.array([review.strip().split('\\t') for review in reviews])\n",
    "        # Conversion to lower case\n",
    "        reviews[:, 1] = np.char.lower(reviews[:, 1])\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7e013ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the List of Positive and negative words\n",
    "def read_words_from_file(filePath):\n",
    "    \"\"\"\n",
    "    Reads and returns set of words read from a file\n",
    "    \"\"\"\n",
    "    with open(filePath) as fp:\n",
    "        words = fp.readlines()\n",
    "        words = set([word.strip() for word in words])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15afcb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the pronouns \n",
    "all_pronouns = [\"i\", \"me\", \"mine\", \"my\", \"you\", \"your\", \"yours\", \"we\", \"us\", \"ours\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0eb0f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive reviews: (95, 2)\n",
      "Negative reviews: (94, 2)\n"
     ]
    }
   ],
   "source": [
    "# Read the positive and negative reviews\n",
    "positive_reviews = read_reviews_from_file(\"hotelPosT-train.txt\")\n",
    "negative_reviews = read_reviews_from_file(\"hotelNegT-train.txt\")\n",
    "\n",
    "# Print the statistics for train data set \n",
    "print(\"Positive reviews:\", positive_reviews.shape)\n",
    "print(\"Negative reviews:\", negative_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c24e008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive words: 2006\n",
      "Negative words: 4780\n",
      "Pronouns: 10\n"
     ]
    }
   ],
   "source": [
    "# Read Positive and Negative words\n",
    "positive_words = read_words_from_file(\"positive-words.txt\")\n",
    "negative_words = read_words_from_file(\"negative-words.txt\")\n",
    "\n",
    "# Print the statistics for Postive, Negative and pronouns\n",
    "print(\"Positive words:\", len(positive_words))\n",
    "print(\"Negative words:\", len(negative_words))\n",
    "print(\"Pronouns:\", len(all_pronouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35cee3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper methods\n",
    "def get_word_counts(stripped_words):\n",
    "    \"\"\"\n",
    "    Returns the count of positive words in the review\n",
    "    \"\"\"\n",
    "    # Count the positive and negative words and pronouns\n",
    "    positives = [word for word in stripped_words if word in positive_words]\n",
    "    negatives = [word for word in stripped_words if word in negative_words]\n",
    "    cur_pronouns = [word for word in stripped_words if word in all_pronouns]\n",
    "    return len(positives), len(negatives), len(cur_pronouns)\n",
    "\n",
    "def check_if_no_present(stripped_words):\n",
    "    \"\"\"\n",
    "    Returns 1 if the word no is present in the review else 0\n",
    "    \"\"\"\n",
    "    is_present = any([word==\"no\" for word in stripped_words])\n",
    "    return 1 if is_present else 0\n",
    "\n",
    "def check_if_exclamation_present(current_words):\n",
    "    \"\"\"\n",
    "    Returns 1 if ! is present in the review else 0\n",
    "    \"\"\"\n",
    "    is_present = any([word.endswith('!') for word in current_words])\n",
    "    return 1 if is_present else 0\n",
    "\n",
    "def process_reviews(reviews, polarity=None):\n",
    "    \"\"\"\n",
    "    Process reviews of a given polarity and return the processed results\n",
    "    \n",
    "    Params: \n",
    "        reviews: Reviews obtained from the training data of a particular polarity\n",
    "        polarity: 1 for Positive and 0 for negative reviews\n",
    "    \"\"\"\n",
    "    processed_reviews = []\n",
    "    for idx, row in enumerate(reviews):\n",
    "        current_words = row[1].split(\" \")\n",
    "    \n",
    "        # Strip the special characters from the words like [,.!]\n",
    "        stripped_words = [word.strip(\"!.,\") for word in current_words]\n",
    "\n",
    "        # Get Positive and negative word counts and pronoun counts\n",
    "        pos_count, neg_count, pronoun_count = get_word_counts(stripped_words)\n",
    "\n",
    "        # Check if \"no\" present in the review\n",
    "        no_present = check_if_no_present(stripped_words)\n",
    "\n",
    "        # Check if \"!\" present in the review\n",
    "        # Note: We should not pass the stripped words for this calculation\n",
    "        exclamation_present = check_if_exclamation_present(current_words)\n",
    "\n",
    "        # Get Log of the word count of the document\n",
    "        log_word_count = round(np.log(len(current_words)),2)\n",
    "\n",
    "        # Current result\n",
    "        # ID, PosCount, NegCount, NoPresent, PronounCount, ExclamationPresent, LogWordCounts, ReviewPolarity\n",
    "        if polarity is not None:\n",
    "            current_result = [row[0], pos_count, neg_count, no_present, pronoun_count, exclamation_present, log_word_count, polarity]\n",
    "        else:\n",
    "            current_result = [row[0], pos_count, neg_count, no_present, pronoun_count, exclamation_present, log_word_count]\n",
    "        \n",
    "        # Append the current result to processed o/p\n",
    "        processed_reviews.append(current_result)\n",
    "    \n",
    "    return np.array(processed_reviews, dtype='object')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58b9afc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Positive and negative reviews\n",
    "processed_positives = process_reviews(positive_reviews, 1)\n",
    "processed_negatives = process_reviews(negative_reviews, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a32caef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Positives: (95, 8)\n",
      "Processed Negatives: (94, 8)\n"
     ]
    }
   ],
   "source": [
    "# Print the statistics for processed reviews\n",
    "print(\"Processed Positives:\", processed_positives.shape)\n",
    "print(\"Processed Negatives:\", processed_negatives.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0270a525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate Positive and Negative processed output\n",
    "all_processed_reviews = np.concatenate((processed_positives, processed_negatives), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a79f18bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Processed Reviews: (189, 8)\n"
     ]
    }
   ],
   "source": [
    "# Print the statistics of the combined reviews\n",
    "print(\"Final Processed Reviews:\", all_processed_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "591d3f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the processed output to a CSV file\n",
    "formatter = ('%s,%d,%d,%d,%d,%d,%.2f,%d')\n",
    "np.savetxt(\"VijayasriMohanKumar-Aravindakumar-assgn2-part1.csv\", all_processed_reviews, delimiter=\",\", fmt=formatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa888d7b",
   "metadata": {},
   "source": [
    "## Part 2 - Training, DevTesting and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ca4c005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper methods:\n",
    "\n",
    "def convert_data_type(data):\n",
    "    \"\"\"\n",
    "    Change the data type of Columns in the CSV data\n",
    "    \"\"\"\n",
    "    # Initialize necessary params for conversion\n",
    "    new_data = np.empty(data.shape, dtype=object)\n",
    "    integer_indices = [1, 2, 3, 4, 5, 7]\n",
    "    float_indices = [6]\n",
    "    string_indices = [0]\n",
    "    \n",
    "    for idx in range(data.shape[1]):\n",
    "        if idx in string_indices:\n",
    "            new_data[:, 0] = data[:, 0]\n",
    "        elif idx in float_indices:\n",
    "            new_data[:, 6] = data[:, 6].astype(np.dtype('float64'))\n",
    "        elif idx in integer_indices:\n",
    "            new_data[:, idx] = data[:, idx].astype(np.dtype('int32'))\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "def read_processed_train_data(filePath):\n",
    "    # Read the processed train data from Part 1\n",
    "    with open(filePath) as fp:\n",
    "        csv_data = fp.readlines()\n",
    "        csv_data = np.array([line.strip().split(',') for line in csv_data])\n",
    "        csv_data = convert_data_type(csv_data)\n",
    "    return csv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "092093c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: (189, 8)\n"
     ]
    }
   ],
   "source": [
    "# Read the processed training data\n",
    "csv_data = read_processed_train_data(\"VijayasriMohanKumar-Aravindakumar-assgn2-part1.csv\")\n",
    "\n",
    "# Print the train data statistics\n",
    "print(\"Train Data:\", csv_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc3b8343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desired Train data set size: 151\n",
      "Total entries for Train : 151\n",
      "Total entries for Dev-Test: 38\n",
      "Intersection between Train and Dev-test: 0\n"
     ]
    }
   ],
   "source": [
    "# Split Train data into 80:20 ratio for training and dev-testing\n",
    "train_ratio = 0.80\n",
    "total_size = len(csv_data)\n",
    "train_size = int(total_size * train_ratio)\n",
    "print(\"Desired Train data set size:\",train_size)\n",
    "\n",
    "# Step 1: Randomize the csv_data to get mixture of positive and negative reviews\n",
    "train_indices = random.sample(range(total_size), train_size)\n",
    "dev_test_indices = list(set(range(total_size)).difference(train_indices))\n",
    "print(\"Total entries for Train :\", len(train_indices))\n",
    "print(\"Total entries for Dev-Test:\", len(dev_test_indices))\n",
    "\n",
    "# Check if there is any overlap between train and dev test data\n",
    "print(\"Intersection between Train and Dev-test:\", len(set(train_indices).intersection(dev_test_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b118d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the entries corresponding to the obtained indices\n",
    "train_data = np.take(csv_data, indices=train_indices, axis=0)\n",
    "dev_test_data = np.take(csv_data, indices=dev_test_indices, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62e2b4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: (151, 8)\n",
      "Dev Test Data: (38, 8)\n",
      "Postive and negative samples in Train     => Pos: 80 Neg: 71\n",
      "Postive and negative samples in Dev Test  => Pos: 15 Neg: 23\n"
     ]
    }
   ],
   "source": [
    "# Print the statistics for train and dev_test\n",
    "print(\"Train Data:\", train_data.shape)\n",
    "print(\"Dev Test Data:\", dev_test_data.shape)\n",
    "\n",
    "pos_count = list(train_data[:,7]).count(1)\n",
    "print(\"Postive and negative samples in Train     =>\", f\"Pos: {pos_count}\", f\"Neg: {len(train_data)-pos_count}\")\n",
    "pos_count = list(dev_test_data[:,7]).count(1)\n",
    "print(\"Postive and negative samples in Dev Test  =>\", f\"Pos: {pos_count}\", f\"Neg: {len(dev_test_data)-pos_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04daa6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into X_train, Y_train and X_dev_test, Y_dev_test\n",
    "def train_test_split(train_data, dev_test_data):\n",
    "    \"\"\"\n",
    "    Splits the data into X_train, Y_train and x_dev_test, y_dev_test\n",
    "    \"\"\"\n",
    "    X_train, Y_train = train_data[:, :7], train_data[:, 7]\n",
    "    X_dev_test, Y_dev_test = dev_test_data[:, :7], dev_test_data[:, 7]\n",
    "    \n",
    "    # Add a dummy feature to train for bias term\n",
    "    X_train = np.append(X_train, np.array([1]*len(X_train)).reshape((-1,1)), axis=1)\n",
    "    X_dev_test = np.append(X_dev_test, np.array([1]*len(X_dev_test)).reshape(-1,1), axis=1)\n",
    "    \n",
    "    return X_train, Y_train, X_dev_test, Y_dev_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c561446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(151, 8) (151,) (38, 8) (38,)\n"
     ]
    }
   ],
   "source": [
    "# Get the train and dev-test split\n",
    "X_train, Y_train, X_dTest, Y_dTest = train_test_split(train_data, dev_test_data)\n",
    "print(X_train.shape, Y_train.shape, X_dTest.shape, Y_dTest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea627953",
   "metadata": {},
   "source": [
    "### SGD Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c5be2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "def get_class_score(z):\n",
    "    \"\"\"\n",
    "    Returns Class score\n",
    "    \"\"\"\n",
    "    # To eliminate numerical underflow increase the byte size.\n",
    "    score = np.float128(1/(1+np.exp(-z)))\n",
    "    return score\n",
    "\n",
    "def perform_SGD(X_train, Y_train, learning_rate=0.01, bias=0.1, epochs=15000):\n",
    "    \"\"\"\n",
    "    Runs SGD and returns final weights and epochs\n",
    "    \"\"\"\n",
    "    # Initialize Epoch\n",
    "    epoch = 0\n",
    "    \n",
    "    # Weights\n",
    "    weights = [0]*6 + [bias]\n",
    "    weights = np.array(weights)\n",
    "    print(\"Initial weights:\", weights, \"Shape:\", weights.shape)\n",
    "    \n",
    "    # Take a copy of the X_train and Y_train\n",
    "    X_train_copy = X_train * 1\n",
    "    Y_train_copy = Y_train * 1\n",
    "    \n",
    "    while epoch <= epochs:\n",
    "    \n",
    "        if len(X_train_copy)==0:\n",
    "            X_train_copy = X_train * 1\n",
    "            Y_train_copy = Y_train * 1\n",
    "\n",
    "        # Choose a random point from train data\n",
    "        random_idx = random.randint(0, len(X_train_copy)-1)\n",
    "\n",
    "        # Get features and label corresponding to the index\n",
    "        features = X_train_copy[random_idx]\n",
    "        actual_score = Y_train_copy[random_idx]\n",
    "\n",
    "        # Remove the entries in current index\n",
    "        # To implement Random sampling without replacement\n",
    "        # This ensures all the data points have been tried atleast once across all epoch.\n",
    "        X_train_copy = np.delete(X_train_copy, random_idx, axis=0)\n",
    "        Y_train_copy = np.delete(Y_train_copy, random_idx)\n",
    "\n",
    "        # Compute the gradient\n",
    "        # Features will also have ID which we exclude in gradient calculation\n",
    "        z = np.dot(weights, features[1:])\n",
    "        predicted_score = get_class_score(z)\n",
    "\n",
    "        # Gradient = (predicted-actual)*features\n",
    "        gradient = (predicted_score - actual_score) * features[1:]\n",
    "\n",
    "        # Get updated weights\n",
    "        new_weights = weights - learning_rate * gradient\n",
    "\n",
    "        # Reassign the weights for next cycle\n",
    "        weights = new_weights\n",
    "\n",
    "        # Increment the epoch\n",
    "        epoch += 1\n",
    "\n",
    "    # end while\n",
    "    \n",
    "    return weights, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d981498c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights: [0.  0.  0.  0.  0.  0.  0.1] Shape: (7,)\n",
      "Epochs completed: 20001\n",
      "Final weights: [0.34583545869863899032 -1.2071607507809708884 -0.30619922632967823387\n",
      " 0.032720586291096234375 0.8045754328993896735 -0.03246593014529219733\n",
      " 0.3450637627091489359]\n"
     ]
    }
   ],
   "source": [
    "# Run SGD\n",
    "weights, epoch = perform_SGD(X_train, Y_train, learning_rate=0.01, bias=0.1, epochs=20000)\n",
    "\n",
    "# Print epochs and final weights\n",
    "print(\"Epochs completed:\", epoch)\n",
    "print(\"Final weights:\", weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58e8937",
   "metadata": {},
   "source": [
    "### Validating the accuracy with Dev Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "224fce12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "\n",
    "# Cross entropy loss calculator\n",
    "def cross_entropy_loss(y, y_hat):\n",
    "    loss = (y * np.log(y_hat)) + ((1-y) * np.log(1-y_hat))\n",
    "    return loss\n",
    "\n",
    "def get_accuracy_and_loss(X_dTest, Y_dTest):\n",
    "    \"\"\"\n",
    "    Compute the accuracy and loss\n",
    "    \"\"\"\n",
    "    # Compute the cross entropy loss\n",
    "    losses = []\n",
    "    correct_preds = 0\n",
    "\n",
    "    for idx, test_feature in enumerate(X_dTest):\n",
    "    \n",
    "        # Get the label\n",
    "        y = Y_dTest[idx]\n",
    "\n",
    "        # Compute the probability\n",
    "        z = np.dot(weights, test_feature[1:])\n",
    "        y_hat = get_class_score(z)\n",
    "\n",
    "        pred = 1 if y_hat > 0.5 else 0\n",
    "        if y == pred:\n",
    "            correct_preds += 1\n",
    "\n",
    "        # Compute the loss\n",
    "        cur_loss = cross_entropy_loss(y, y_hat)\n",
    "\n",
    "        # Append to all loss\n",
    "        losses.append(cur_loss)\n",
    "    \n",
    "    N = len(X_dTest)\n",
    "    average_loss = -(1/N) * sum(losses)\n",
    "    accuracy = (correct_preds/N)*100\n",
    "    \n",
    "    return correct_preds, average_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48fbc29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics on Dev Test Data Set:\n",
      "Total correct classifications: 34\n",
      "Accuracy: 89.47368421052632\n",
      "Cross Entropy loss: 0.18937812968517787569\n"
     ]
    }
   ],
   "source": [
    "# Test the model and DEV dataset and report the accuracy\n",
    "print(\"Metrics on Dev Test Data Set:\")\n",
    "correct_preds, cp_loss, accuracy = get_accuracy_and_loss(X_dTest, Y_dTest)\n",
    "print(\"Total correct classifications:\", correct_preds)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Cross Entropy loss:\", cp_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f7393ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics on Train Data Set:\n",
      "Total correct classifications: 139\n",
      "Accuracy: 92.05298013245033\n",
      "Cross Entropy loss: 0.26378815408978323995\n"
     ]
    }
   ],
   "source": [
    "# Metrics on Train Data Set\n",
    "print(\"Metrics on Train Data Set:\")\n",
    "correct_preds, cp_loss, accuracy = get_accuracy_and_loss(X_train, Y_train)\n",
    "print(\"Total correct classifications:\", correct_preds)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Cross Entropy loss:\", cp_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2984066",
   "metadata": {},
   "source": [
    "### Evaluating the model on Actual Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b26f5490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Reviews: (50, 2)\n"
     ]
    }
   ],
   "source": [
    "# Read the test reviews from the file\n",
    "# Replace the file name on getting the test data\n",
    "test_reviews = read_reviews_from_file(\"HW2-testset.txt\")\n",
    "print(\"Test Reviews:\", test_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4124faf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurized Test Data: (50, 7)\n"
     ]
    }
   ],
   "source": [
    "# Featurize the reviews \n",
    "X_test = process_reviews(test_reviews)\n",
    "print(\"Featurized Test Data:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efa35616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each test data compute the probability a using the weights obtained above in SGD\n",
    "def predict(X_test, weights):\n",
    "    \"\"\"\n",
    "    Predicts the polarity based on the weights obtained\n",
    "    \"\"\"\n",
    "    # Add 1 as Column for bias\n",
    "    X_test = np.append(X_test, np.array([1]*len(X_test)).reshape((-1,1)), axis=1)\n",
    "    print(\"Shape after adding Bias column:\", X_test.shape)\n",
    "    \n",
    "    result = []\n",
    "    for review in X_test:\n",
    "        # Get Predicted Score\n",
    "        predicted_score = get_class_score(np.dot(weights, review[1:]))\n",
    "        \n",
    "        # Decide class based on Probability\n",
    "        if (predicted_score <= 0.5):\n",
    "            class_label = \"NEG\"\n",
    "        else:\n",
    "            class_label = \"POS\"\n",
    "        \n",
    "        # Append to result\n",
    "        result.append([review[0], class_label])\n",
    "        \n",
    "    result = np.array(result, dtype='object')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44157a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after adding Bias column: (50, 8)\n",
      "Predicted Polarities: (50, 2)\n"
     ]
    }
   ],
   "source": [
    "# Predict the polarity of the reviews\n",
    "predicted_polarities = predict(X_test, weights)\n",
    "print(\"Predicted Polarities:\", predicted_polarities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df9daf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to a file in the given format\n",
    "np.savetxt(\"VijayasriMohanKumar-Aravindakumar-assgn2-out.txt\", predicted_polarities, delimiter=\"\\t\", fmt=\"%s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
